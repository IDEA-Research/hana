{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from decoder.model_creation import create_model_and_diffusion as create_model_and_diffusion_dalle2\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "# set device\n",
    "th.cuda.set_device(0)\n",
    "device = th.device('cuda') if th.cuda.is_available() else th.device('cpu')\n",
    "\n",
    "# choose model config and checkpoint\n",
    "config = OmegaConf.load('../config/tiny64_t5_clip.yaml')\n",
    "st_path = '/comp_robot/mm_generative/ckpt/base64/t5_w_clip/model-step=150000.ckpt/checkpoint/mp_rank_00_model_states.pt'\n",
    "\n",
    "# model helper functions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)//1000000\n",
    "\n",
    "use_fp16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text2ImUNet(\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 768, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 768, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 768, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (10): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (11): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (12): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (13): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(576, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(576, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (14): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (15): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (1): Identity()\n",
       "        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "      (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttention()\n",
       "      (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "      (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (1): Identity()\n",
       "        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (output_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1344, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(1344, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1344, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1344, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(1344, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1344, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1152, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(1152, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1152, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(1152, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(960, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(960, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 768, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 768, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (10): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 768, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (11): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(576, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (encoder_kv): Conv1d(1024, 768, kernel_size=(1,), stride=(1,))\n",
       "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (12): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (13): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (14): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (15): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): Identity()\n",
       "          (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
       "    (1): Identity()\n",
       "    (2): Conv2d(192, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (clip_embed_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (transformer_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (time_to_half): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (clip_to_half): Linear(in_features=768, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load decoder model\n",
    "\n",
    "model_and_diffusion_settings = OmegaConf.merge(config.MODEL, config.DIFFUSION)\n",
    "# set model fp16/32\n",
    "model_and_diffusion_settings.use_fp16 = use_fp16\n",
    "del model_and_diffusion_settings[\"schedule_sampler\"]\n",
    "del model_and_diffusion_settings[\"use_clip_emb\"]\n",
    "\n",
    "# load pre-trained u-net and diffusion\n",
    "model, diffusion = create_model_and_diffusion_dalle2(**model_and_diffusion_settings)\n",
    "if use_fp16:\n",
    "    model = model.half()\n",
    "\n",
    "# load ckpt\n",
    "st = torch.load(st_path, map_location='cpu')\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "for name in list(model_state_dict.keys()):\n",
    "    model_state_dict[name] = st['module']['module.model.' + name]\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "unet = model\n",
    "unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionPrior(\n",
       "  (noise_scheduler): NoiseScheduler()\n",
       "  (clip): OpenAIClipAdapter(\n",
       "    (clip): CLIP(\n",
       "      (visual): VisionTransformer(\n",
       "        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (transformer): Transformer(\n",
       "          (resblocks): Sequential(\n",
       "            (0): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (12): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (13): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (14): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (15): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (16): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (17): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (18): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (19): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (20): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (21): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (22): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (23): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (clip_normalize): Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       "  )\n",
       "  (net): DiffusionPriorNetwork(\n",
       "    (to_text_embeds): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Rearrange('b (n d) -> b n d', n=1)\n",
       "    )\n",
       "    (to_time_embeds): Sequential(\n",
       "      (0): Embedding(1000, 768)\n",
       "      (1): Rearrange('b (n d) -> b n d', n=1)\n",
       "    )\n",
       "    (to_image_embeds): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Rearrange('b (n d) -> b n d', n=1)\n",
       "    )\n",
       "    (text_encodings_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (causal_transformer): CausalTransformer(\n",
       "      (rel_pos_bias): RelPosBias(\n",
       "        (relative_attention_bias): Embedding(32, 32)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (12): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (13): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (14): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (15): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (16): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (17): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (18): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (19): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (20): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (21): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (22): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (23): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (to_kv): Linear(in_features=768, out_features=48, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (2): GELU()\n",
       "            (3): LayerNorm()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "      (project_out): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Load prior model\n",
    "\n",
    "import torch\n",
    "import math\n",
    "sys.path.append('/comp_robot/mm_generative/code/dalle2_prior/')\n",
    "config = OmegaConf.load('/comp_robot/mm_generative/code/dalle2_prior/configs/prior.yaml')\n",
    "from dalle2_pytorch import DiffusionPrior, DiffusionPriorNetwork, OpenAIClipAdapter\n",
    "torch.pi = math.pi\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "prior_cfg = config.prior\n",
    "        \n",
    "net = DiffusionPriorNetwork(**prior_cfg.net)\n",
    "del prior_cfg[\"net\"]\n",
    "\n",
    "my_clip = OpenAIClipAdapter(name=prior_cfg.clip.model)\n",
    "del prior_cfg[\"clip\"]\n",
    "prior = DiffusionPrior(\n",
    "    net=net,\n",
    "    clip=my_clip,\n",
    "    **prior_cfg,\n",
    ")\n",
    "# load ckpt\n",
    "st_path = '/comp_robot/mm_generative/ckpt/diffusion_prior/model-step=400000.ckpt/checkpoint/mp_rank_00_model_states.pt'\n",
    "st = torch.load(st_path, map_location='cpu')\n",
    "\n",
    "model = prior\n",
    "model_state_dict = model.state_dict()\n",
    "for name in list(model_state_dict.keys()):\n",
    "    model_state_dict[name] = st['module']['module.prior.' + name]\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Sample drawbench prompts.\n",
    "from read_eval import EvalDataset\n",
    "eval_ds = EvalDataset('/comp_robot/mm_generative/eval/prompts/drawbench/drawbench.tsv',\n",
    "                      '/comp_robot/mm_generative/eval/prompts/drawbench/drawbench.lineidx')\n",
    "\n",
    "\n",
    "t5_list,  clip_text_list, meta_list = [], [], []\n",
    "selected_prompts = [\n",
    "# color\n",
    "'A pink colored car.',\n",
    "'A black apple and a green backpack.',\n",
    "# conflicting\n",
    "'A shark in the desert.',\n",
    "'A panda making latte art.',\n",
    "# counting\n",
    "'Five cars on the street.',\n",
    "'One cat and one dog sitting on the grass.',\n",
    "# dalle\n",
    "'A couple of glasses are sitting on a table.',\n",
    "'A small blue book sitting on a large red book.',\n",
    "# descriptions\n",
    "'A large plant-eating domesticated mammal with solid hoofs and a flowing mane and tail, used for riding, racing, and to carry and pull loads.',\n",
    "'A machine next to a parking space in a street, into which the driver puts money so as to be authorized to park the vehicle for a particular length of time.',\n",
    "# Gary Marcus et al. \n",
    "'A donkey and an octopus are playing a game. The donkey is holding a rope on one end, the octopus is holding onto the other. The donkey holds the rope in its mouth. A cat is jumping over the rope.',\n",
    "'An oil painting of a couple in formal evening wear going home get caught in a heavy downpour with no umbrellas.',\n",
    "# Misspellings\n",
    "'Dininrg tablez.',\n",
    "'An instqrumemnt used for cutting cloth, paper, axdz othr thdin mteroial, consamistng of two blades lad one on tvopb of the other and fhastned in tle mixdqdjle so as to bllow them txo be pened and closed by thumb and fitngesr inserted tgrough rings on kthe end oc thei vatndlzes.',\n",
    "# Positional\n",
    "'A banana on the left of an apple.',\n",
    "'A hair drier underneath a sheep.',\n",
    "# Rare Words\n",
    "'Acersecomicke.',\n",
    "'Octothorpe.',\n",
    "# Reddit\n",
    "'A 1960s poster warning against climate change.',\n",
    "'A tiger in a lab coat with a 1980s Miami vibe, turning a well oiled science content machine, digital art.',\n",
    "# Text\n",
    "\"\"\"A sign that says 'Deep Learning'.\"\"\",\n",
    "\"\"\"A sign that says 'Hello World'.\"\"\"\n",
    "]\n",
    "\n",
    "for idx in range(len(eval_ds)):\n",
    "    (t5_encoding, clip_text_embedding, _), meta = eval_ds.get_row(idx)\n",
    "    if meta['text'] in selected_prompts:\n",
    "        t5_list.append(t5_encoding)\n",
    "        clip_text_list.append(clip_text_embedding)\n",
    "        meta_list.append(meta)\n",
    "\n",
    "import numpy as np\n",
    "t5_encoding = np.stack(t5_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Or, get self-created prompts.\n",
    "from read_eval import EvalDataset\n",
    "eval_ds = EvalDataset('prompt_processing/eval.tsv',\n",
    "                      'prompt_processing/eval.lineidx')\n",
    "\n",
    "\n",
    "t5_list,  clip_text_list, meta_list, t5_mask_list = [], [], [], []\n",
    "for idx in range(len(eval_ds)):\n",
    "    (t5_encoding, clip_text_embedding, _), meta = eval_ds.get_row(idx)\n",
    "    mask = np.zeros(t5_encoding.shape[0], dtype=np.int)\n",
    "    num_valid_text_tokens = int(meta['t5_num_valid_tokens'])\n",
    "    mask[:num_valid_text_tokens] = 1\n",
    "    t5_list.append(t5_encoding)\n",
    "    clip_text_list.append(clip_text_embedding)\n",
    "    meta_list.append(meta)\n",
    "    t5_mask_list.append(mask)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "pretrained_text_encodings, clip_text_embed, pretrained_text_mask = map(lambda x: torch.from_numpy(np.stack(x)), \n",
    "                                                                        [t5_list, clip_text_list, t5_mask_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clip embedding functions\n",
    "# Not used for now\n",
    "import clip\n",
    "from utils.tokenizer import get_encoder\n",
    "import torchvision.transforms as T\n",
    "tokenizer = get_encoder(text_ctx=256)\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "\n",
    "def get_clip_text_embedding(texts, device):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    tokens, masks, text_embs = [], [], []\n",
    "    for text in texts:\n",
    "        token, mask = tokenizer(text)\n",
    "        token, mask = map(lambda t: torch.tensor(t).unsqueeze(0), [token, mask])\n",
    "        \n",
    "        token4clip = clip.tokenize(text, truncate=True).to(device)\n",
    "        text_emb = clip_model.encode_text(token4clip)\n",
    "        \n",
    "        text_emb, token, mask = map(lambda t: t.to(device), [text_emb, token, mask])\n",
    "\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        text_embs.append(text_emb)\n",
    "    return map(lambda ts: torch.cat(ts, dim=0), (tokens, masks, text_embs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.1165, device='cuda:0') tensor(8.7370, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 4. Get image embedding from prior\n",
    "cond_scale = 1.0\n",
    "sample_timesteps = 64\n",
    "\n",
    "clip_text_embed, pretrained_text_encodings, pretrained_text_mask = list(\n",
    "                    map(lambda x: x.to(device), [clip_text_embed, pretrained_text_encodings, pretrained_text_mask]))\n",
    "\n",
    "text_cond = dict(\n",
    "    text_embed = clip_text_embed,\n",
    "    text_encodings = pretrained_text_encodings,\n",
    "    mask = pretrained_text_mask,\n",
    ")\n",
    "predicted_image_embed = prior.p_sample_loop(\n",
    "    shape = clip_text_embed.shape,\n",
    "    text_cond = text_cond,\n",
    "    timesteps = sample_timesteps,\n",
    "    cond_scale = cond_scale,\n",
    ")\n",
    "\n",
    "print(predicted_image_embed.min(), predicted_image_embed.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 22.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A beautiful day.', 'A peaceful night in the desert watching stars.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x128 at 0x7F7670629DC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. image embedding to image\n",
    "\n",
    "image_embed = predicted_image_embed.to(device)\n",
    "mask = pretrained_text_mask.to(device)\n",
    "pretrained_text_encodings = pretrained_text_encodings.to(device)\n",
    "\n",
    "guidance_scale = 10.\n",
    "\n",
    "batch_size = image_embed.shape[0]\n",
    "full_batch_size = batch_size * 2\n",
    "image_size = 64\n",
    "\n",
    "out_dict = {}\n",
    "out_dict[\"text_encodings\"] = th.cat(\n",
    "    [pretrained_text_encodings, th.zeros_like(pretrained_text_encodings)], dim=0)\n",
    "out_dict[\"clip_emb\"] = th.cat(\n",
    "    [image_embed, th.zeros_like(image_embed)], dim=0)\n",
    "\n",
    "\n",
    "dynamic_thresholding_percentile = None\n",
    "\n",
    "def model_fn(x_t, ts, **kwargs):\n",
    "    global guidance_scale\n",
    "    guidance_scale = guidance_scale\n",
    "    half = x_t[: len(x_t) // 2]\n",
    "    combined = th.cat([half, half], dim=0)\n",
    "    model_out = unet(combined, ts, **kwargs)\n",
    "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "    cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
    "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
    "    eps = th.cat([half_eps, half_eps], dim=0)\n",
    "    return th.cat([eps, rest], dim=1)\n",
    "\n",
    "sample = diffusion.p_sample_loop(\n",
    "    model_fn,\n",
    "    shape=(full_batch_size, 3, image_size, image_size),\n",
    "    clip_denoised=True,\n",
    "    dynamic_threshold=dynamic_thresholding_percentile,\n",
    "    model_kwargs=out_dict,\n",
    "    device=device,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "sample = sample[:batch_size]\n",
    "grid = make_grid(sample, nrow=int(\n",
    "        math.sqrt(batch_size)), padding=0).cpu()\n",
    "image = to_pil_image(grid.add(1).div(2).clamp(0, 1))  # PIL.Image\n",
    "\n",
    "captions = [meta[\"text\"] for meta in meta_list]\n",
    "print(captions)\n",
    "image.save(f'prior_scale_{cond_scale}_guidance_{guidance_scale}.png')\n",
    "sample = sample.cpu().numpy()\n",
    "np.save(f'prior_scale_{cond_scale}_guidance_{guidance_scale}.npy', sample)\n",
    "\n",
    "image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mm2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d41cd17e5e724099c922b9ad60073a7482e3facfadf9d0f4e78f454120364e79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
