MODEL:
  image_size: 64
  num_channels: 512
  num_res_blocks: 3 # depth
  channel_mult: "1,2,3,4"
  num_heads: 1
  num_head_channels: 64
  num_heads_upsample: -1
  attention_resolutions: "32,16,8"
  dropout: 0.
  use_pretrained_text_encoder: True
  text_ctx: 256 # text tokens length (text encoder context)
  xf_width:  1024 # text encoder width [wo text encoder]
  xf_layers: 16 # text encoder depth
  xf_heads: 32  # text encoder heads
  xf_final_ln: True
  xf_padding: True
  use_scale_shift_norm: True
  resblock_updown: True
  use_fp16: True # change frequently
  cache_text_emb: False
  inpaint: False
  super_res: False
  use_clip_emb: False

DIFFUSION:
  # diffusion
  learn_sigma: False
  sigma_small: False
  diffusion_steps: 1000
  noise_schedule: "cosine"
  timestep_respacing: ""
  eval_timestep_respacing: "250"
  use_kl: False
  predict_xstart: False
  rescale_timesteps: True
  rescale_learned_sigmas: True
  schedule_sampler: null

TRAIN:
  seed: 0
  # ema
  ema: False
  ema_decay: 0.9999
  cpu: False
  update_ema_after_steps: 0
  update_every_steps: 1
  # common
  train_micro_batch_size_per_gpu: 1
  lr: 1e-4
  min_lr: 0.  # Imagen usues cosine decay learning rate. Not supported by deepspeed.
  warmup_num_steps: 10000
  betas: [0.9, 0.99]
  eps: 1e-8
  weight_decay: 0
  # trainer
  trainer:
    max_epochs: 15
    max_steps: null  # Imagen 2.5M x 2048
    # multi-gpu
    gpus: 1
    num_nodes: 1
    # gradient
    gradient_clip_val: null
    accumulate_grad_batches: 1
    # log
    log_every_n_steps: 1
    # ckpt
    ckpt_path: null

  # log
  wandb:
    project: "idea_art"
    name: "t5_LAION400"
  # checkpoint
  checkpoint:
    every_n_train_steps: 10000
    dirpath: "/comp_robot/mm_generative/ckpt/idea_art/t5_v2"
    filename: "model-{step:02d}"
    save_last: True
    save_top_k: -1
    save_weights_only: False
  # demo / val
  demo:
    every: 2000
    dynamic_thresholding_percentile: 0.9
    guidance_scale: 5.0
    online: True  # whether to eval online model.

DATA:
  dataname: "LAION_400M"
  copy2local: True
  image_size: 64
  batch_size: 1 # TRAIN.train_micro_batch_size_per_gpu 
  val_batch_size: 4
  num_workers: 8
  text_ctx: 256
  interpolation: "bilinear" #["linear", "bilinear", "bicubic", "lanczos"]
  p_flip: 0.5
  use_tokenizer: False # set True if not use pre-trained text encoder
  use_pretrained_text_encoder: True # same as MODEL.use_pretrained_text_encoder