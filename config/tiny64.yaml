MODEL:
  image_size: 64
  num_channels: 192
  num_res_blocks: 3 # depth
  channel_mult: "1,2,3,4"
  num_heads: 1
  num_head_channels: 64
  num_heads_upsample: -1
  attention_resolutions: "32,16,8"
  dropout: 0.1
  use_pretrained_text_encoder: False
  text_ctx: 256 # text tokens length (text encoder context)
  xf_width:  1024 # text encoder width [wo text encoder]
  xf_layers: 16 # text encoder depth
  xf_heads: 32  # text encoder heads
  xf_final_ln: True
  xf_padding: True
  use_scale_shift_norm: True
  resblock_updown: True
  use_fp16: True # change frequently
  cache_text_emb: False
  inpaint: False
  super_res: False
  use_clip_emb: True

DIFFUSION:
  # diffusion
  learn_sigma: True
  sigma_small: False
  diffusion_steps: 1000
  noise_schedule: "cosine"
  timestep_respacing: ""
  eval_timestep_respacing: "250"
  use_kl: False
  predict_xstart: False
  rescale_timesteps: True
  rescale_learned_sigmas: True
  schedule_sampler: null

TRAIN:
  seed: 0
  # ema
  ema: True
  ema_decay: 0.9999
  cpu: False
  update_ema_after_steps: 1000
  update_every_steps: 1
  # common
  train_micro_batch_size_per_gpu: 1
  lr: 1.2e-4
  min_lr: 1e-5
  warmup_num_steps: 1000
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 1e-9
  # trainer
  trainer:
    max_epochs: 50
    max_steps: null
    # multi-gpu
    gpus: 1
    num_nodes: 1
    # gradient
    gradient_clip_val: 1.0
    accumulate_grad_batches: 1
    # log
    log_every_n_steps: 1
    # ckpt
    ckpt_path: null

  # log
  wandb:
    project: "tiny64"
    name: "naive"
  # checkpoint
  checkpoint:
    every_n_train_steps: 50000
    dirpath: "./ckpt/base64"
    filename: "model-{step:02d}"
    save_last: True
    save_top_k: -1
    save_weights_only: False
  # demo / val
  demo:
    every: 250
    dynamic_thresholding_percentile: Null
    guidance_scale: 3.0
    online: False  # whether to eval online model.

DATA:
  dataname: "LAION_400M"
  copy2local: True
  image_size: 64
  batch_size: 1 # TRAIN.train_micro_batch_size_per_gpu 
  val_batch_size: 4
  num_workers: 4
  text_ctx: 256
  interpolation: "bilinear" #["linear", "bilinear", "bicubic", "lanczos"]
  p_flip: 0.5
  use_tokenizer: False # set True if not use pre-trained text encoder
  use_pretrained_text_encoder: False # same as MODEL.use_pretrained_text_encoder